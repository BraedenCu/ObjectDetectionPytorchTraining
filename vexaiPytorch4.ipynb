{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vexaiPytorch4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyObCrseIe0qYLn7eVvCx43u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BraedenCu/ObjectDetectionWithPytorchTutorial/blob/master/vexaiPytorch4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qt8uw-rTC-N",
        "colab_type": "text"
      },
      "source": [
        "Import necessary packages (should be automatically installed in colab)\n",
        "If you get an import error, install each of them using python3 -m pip install ____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8gdyAcwTQzI",
        "colab_type": "text"
      },
      "source": [
        "Next we clone my fork of the pytorch vision repository, which contains scripts we will use later. Then clone some useful scripts for data manipluation from my repository. Finally, create directories for annotations and images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKcuIruY6HrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#EVERYTIME KERNEL DIES, YOU NEED TO REUPLOAD XMLS AND IMAGES\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import pycocotools\n",
        "import os\n",
        "\n",
        "%cd /root/\n",
        "#need to clone my fork, elsewise it will throw import error when using one of the libraries\n",
        "#remove and previous installation, clone the most recent\n",
        "!rm -r vision\n",
        "!git clone https://github.com/BraedenCu/vision.git\n",
        "!cd vision\n",
        "!git checkout v0.6.0\n",
        "\n",
        "%cd ~\n",
        "#remove any previous clone of the repo and install the latest release \n",
        "!rm -r scriptsForObjectDetectionDataPreparation/\n",
        "!git clone https://github.com/BraedenCu/scriptsForObjectDetectionDataPreparation.git\n",
        "!mkdir annotations\n",
        "!mkdir images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erGlgp2JTmD7",
        "colab_type": "text"
      },
      "source": [
        "Here we are first removing any prevois copys of the following scripts and updating them. These scripts will be used later for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2471OV_3CH_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#in order to gain access to functions defined in engine.py/utils.py/etc. we need to copy\n",
        "#the python scripts to root\n",
        "%cd /root/\n",
        "#remove old versions of the python scripts and replace with updated ones\n",
        "!rm -r engine.py\n",
        "!rm -r coco_eval.py\n",
        "!rm -r coco_utils.py\n",
        "!rm -r utils.py\n",
        "%cd vision\n",
        "%cd references\n",
        "%cd detection\n",
        "#copy files to root\n",
        "!cp engine.py -b /root/\n",
        "!cp utils.py -b /root/\n",
        "!cp coco_utils.py -b /root/\n",
        "!cp coco_eval.py -b /root/\n",
        "#change directory back to root\n",
        "%cd /root/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB2PugJl-r5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd ~/scriptsForObjectDetectionDataPreparation/\n",
        "#convert xml_to_csv, if it failes to create do to empty csv, be sure to check that you have uploaded your images and annottions\n",
        "#remove and reupdate\n",
        "!rm -r vex_labels.csv\n",
        "!python xml_to_csv.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgY60oZu-wDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "#function to return data when given filename\n",
        "def parse_one_annot(path_to_csv_file, filename):\n",
        "   data = pd.read_csv(path_to_csv_file)\n",
        "   boxes_array = data[data[\"filename\"] == filename][[\"xmin\", \"ymin\",        \n",
        "   \"xmax\", \"ymax\"]].values\n",
        "   #print(boxes_array)\n",
        "   return boxes_array\n",
        "\n",
        "   \n",
        "class vex(object):\n",
        "    def __init__(self, root, transforms):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = \"/root/images\" #sorted(os.listdir(os.path.join(root, \"images\")))\n",
        "        self.csv_path = \"/root/scriptsForObjectDetectionDataPreparation/vex_labels.csv\"\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images ad masks\n",
        "        #simply just repeat 1 rather than search for 0.jpeg, which does not exist\n",
        "        if idx == 0:\n",
        "          idx = 1\n",
        "        \n",
        "        img_path = \"/root/images/{}{}\".format(idx, \".jpeg\")\n",
        "        #print(img_path)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        \n",
        "        # convert the PIL Image into a numpy array\n",
        "        #box_list = parse_one_annot(self.csv_path, self.imgs[idx])\n",
        "        box_list = parse_one_annot(self.csv_path, img_path)\n",
        "        boxes = torch.as_tensor(box_list, dtype=torch.float32)\n",
        "        num_objs = len(box_list)\n",
        "        # 5 classes, so five boxes to be either zero or one\n",
        "        num_objs = 5\n",
        "        labels = torch.ones((num_objs), dtype=torch.int64)\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:,0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "    \n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        #target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        #if self.transforms is not None:\n",
        "        img = self.transforms(img)\n",
        "            #target = self.transforms(img)\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        #return len(self.imgs)\n",
        "        return 204"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXUJWHIdhFpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#makes sure image path is correct\n",
        "#img_path = \"/root/images/{}{}\".format(idx, \".jpeg\")\n",
        "#print(img_path)\n",
        "#img = Image.open(img_path).convert(\"RGB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w34XkHXvBjrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# load a model pre-trained pre-trained on COCO\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# replace the classifier with a new one, that has\n",
        "# num_classes which is user-defined\n",
        "num_classes = 5  # 1 class (person) + background\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN95PMquB2B8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "# load a pre-trained model for classification and return\n",
        "# only the features\n",
        "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "# FasterRCNN needs to know the number of\n",
        "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
        "# so we need to add it here\n",
        "backbone.out_channels = 1280\n",
        "\n",
        "# let's make the RPN generate 5 x 3 anchors per spatial\n",
        "# location, with 5 different sizes and 3 different aspect\n",
        "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
        "# map could potentially have different sizes and\n",
        "# aspect ratios\n",
        "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "# let's define what are the feature maps that we will\n",
        "# use to perform the region of interest cropping, as well as\n",
        "# the size of the crop after rescaling.\n",
        "# if your backbone returns a Tensor, featmap_names is expected to\n",
        "# be [0]. More generally, the backbone should return an\n",
        "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
        "# feature maps to use.\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
        "                                                output_size=7,\n",
        "                                                sampling_ratio=2)\n",
        "\n",
        "# put the pieces together inside a FasterRCNN model\n",
        "model = FasterRCNN(backbone,\n",
        "                   num_classes=2,\n",
        "                   rpn_anchor_generator=anchor_generator,\n",
        "                   box_roi_pool=roi_pooler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYjvCltYfhDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model_instance_segmentation(num_classes):\n",
        "   # load an object detection model pre-trained on COCO\n",
        "   model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "  # get the number of input features for the classifier\n",
        "   in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "   # replace the pre-trained head with a new on\n",
        "   model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "   \n",
        "   return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjL3M4nlCBz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import transforms as T\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    #if train:\n",
        "        #transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddplJWutCW63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import utils\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "dataset = vex('/root/', get_transform(train=True))\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        " dataset, batch_size=2, shuffle=True, num_workers=4,\n",
        " collate_fn=utils.collate_fn)\n",
        "# For Training\n",
        "images,targets = next(iter(data_loader))\n",
        "images = list(image for image in images)\n",
        "targets = [{k: v for k, v in t.items()} for t in targets]\n",
        "output = model(images,targets)   # Returns losses and detections\n",
        "# For inference\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)           # Returns predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlm1WB93eVYD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5561194d-4176-44e5-d5c8-ef26ef5edaa6"
      },
      "source": [
        "%cd /root"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K61a-WhAX7k2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "\n",
        "def main():\n",
        "    # train on the GPU or on the CPU, if a GPU is not available\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    # our dataset has two classes only - background and person\n",
        "    num_classes = 5\n",
        "    # use our dataset and defined transformations\n",
        "    dataset = vex('/root/', get_transform(train=True))\n",
        "    dataset_test = vex('/root/', get_transform(train=False))\n",
        "\n",
        "    # split the dataset in train and test set\n",
        "    indices = torch.randperm(len(dataset)).tolist()\n",
        "    #dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
        "    #dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "\n",
        "    # define training and validation data loaders\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=2, shuffle=True, num_workers=4,\n",
        "        collate_fn=utils.collate_fn)\n",
        "\n",
        "    data_loader_test = torch.utils.data.DataLoader(\n",
        "        dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "        collate_fn=utils.collate_fn)\n",
        "\n",
        "    # get the model using our helper function\n",
        "    model = get_model_instance_segmentation(num_classes)\n",
        "\n",
        "    # move model to the right device\n",
        "    model.to(device)\n",
        "\n",
        "    # construct an optimizer\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                                momentum=0.9, weight_decay=0.0005)\n",
        "    # and a learning rate scheduler\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                   step_size=3,\n",
        "                                                   gamma=0.1)\n",
        "\n",
        "    # let's train it for 10 epochs\n",
        "    num_epochs = 5\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # train for one epoch, printing every 10 iterations\n",
        "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "        # update the learning rate\n",
        "        lr_scheduler.step()\n",
        "        # evaluate on the test dataset\n",
        "        evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "    print(\"That's it!\")\n",
        "\n",
        "main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgE5wa4R8X9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save model to training/models\n",
        "#remove prior saves\n",
        "!rm -r training\n",
        "!mkdir training\n",
        "torch.save(model.state_dict(), \"/root/training/model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1QrQUzc-IdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import ImageDraw\n",
        "\n",
        "loaded_model = get_model_instance_segmentation(num_classes = 5)\n",
        "loaded_model.load_state_dict(torch.load(\"/root/training/model\"))\n",
        "dataset_test = vex('/root/', get_transform(train=False))\n",
        "idx = 0\n",
        "img, _ = dataset_test[idx]\n",
        "label_boxes = np.array(dataset_test[idx][1][\"boxes\"])\n",
        "#put the model in evaluation mode\n",
        "loaded_model.eval()\n",
        "with torch.no_grad():\n",
        "   prediction = loaded_model([img])\n",
        "image = Image.fromarray(img.mul(255).permute(1, 2,0).byte().numpy())\n",
        "draw = ImageDraw.Draw(image)\n",
        "# draw groundtruth\n",
        "for elem in range(len(label_boxes)):\n",
        "   draw.rectangle([(label_boxes[elem][0], label_boxes[elem][1]),\n",
        "   (label_boxes[elem][2], label_boxes[elem][3])], \n",
        "   outline =\"green\", width =3)\n",
        "for element in range(len(prediction[0][\"boxes\"])):\n",
        "   boxes = prediction[0][\"boxes\"][element].cpu().numpy()\n",
        "   score = np.round(prediction[0][\"scores\"][element].cpu().numpy(),\n",
        "                    decimals= 4)\n",
        "   if score > 0.45:\n",
        "      draw.rectangle([(boxes[0], boxes[1]), (boxes[2], boxes[3])], \n",
        "      outline =\"red\", width =3)\n",
        "      draw.text((boxes[0], boxes[1]), text = str(score))\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}